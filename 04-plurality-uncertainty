=================================================
Chapter 4: From Plurality to Uncertainty
=================================================

Construction of Number Systems
-------------------------------

Number systems are constructed from simpler ones using equivalence relations.

Integers Z = {..., -2, -1, 0, 1, 2, ...}
  Construction: equivalence classes on N CROSS N

Rationals Q = {p/q | p, q in Z, q != 0}
  Construction: equivalence classes on Z CROSS (Z - {0})

Reals R (all points on the number line)
  Construction: Dedekind cuts or Cauchy sequences of rationals


Algebraic Structures
--------------------

An algebraic structure is a set equipped with one or more operations satisfying
specific axioms.

Formal definition:
  (S, *) where S is a set and * : S CROSS S -> S is a binary operation

Different axioms define different structures: monoids, groups, rings, fields.


Monoids
-------

A monoid is a set with an associative binary operation and an identity element.

Definition:
  (M, *, e) where:
    * : M CROSS M -> M       (binary operation)
    FOR ALL a, b, c IN M:
      (a * b) * c = a * (b * c)     (associativity)
      e * a = a * e = a             (identity)

Monoids are a fundamental structure in abstract algebra, along with groups,
rings, and fields.

Examples:
  (N, +, 0)     - natural numbers under addition
  (N, *, 1)     - natural numbers under multiplication
  (Z, +, 0)     - integers under addition
  (Q, +, 0)     - rationals under addition
  (Q - {0}, *, 1) - nonzero rationals under multiplication
  (R, +, 0)     - reals under addition
  (sequences over A, concatenation, empty sequence)
  (lists over A, concatenation, empty list)


Limits and Convergence
----------------------

Limit of a sequence:
  A sequence (s_n) has limit L if:
  FOR ALL epsilon > 0, EXISTS N, FOR ALL n >= N, dist(s_n, L) < epsilon
  
  where dist(a, b) is the distance between a and b.
  
  Notation: LIM (n -> infinity) s_n = L
  
  The sequence gets arbitrarily close to L for sufficiently large n.

Convergence:
  A sequence converges if it has a finite limit.
  A series SUM(a_n) converges if its partial sums converge.


Sample Spaces and Events
-------------------------

A probability space consists of:
  Sample space Omega - set of all possible outcomes
  Event - subset of Omega

Example:
  Coin flip: Omega = {H, T}
  Die roll: Omega = {1, 2, 3, 4, 5, 6}


Probability Function
--------------------

A probability function P assigns probabilities to events.

Axioms:
  P(Omega) = 1
  P(A) >= 0 for all events A
  P(A UNION B) = P(A) + P(B) when A, B are disjoint


Conditional Probability
-----------------------

Conditional probability of A given B:
  P(A | B) = P(A AND B) / P(B)

Independence:
  Events A and B are independent if P(A AND B) = P(A) * P(B)


Random Variables
----------------

A random variable is a function from the sample space to real numbers.

Formal definition:
  X: Omega -> R

Probability mass function for discrete random variable X:
  p(x) = P(X = x)  for each value x

Properties:
  SUM p(x) = 1  (over all possible values x)
  p(x) >= 0  for all x


Expected Value
--------------

The expected value (mean) of discrete random variable X:
  E[X] = SUM(xi * p(xi))
  
where the sum is over all values xi with p(xi) > 0.


Variance
--------

The variance of discrete random variable X:
  Var(X) = E[(X - E[X])^2] = E[X^2] - (E[X])^2
         = SUM((xi - E[X])^2 * p(xi))

Standard deviation:
  SD(X) = sqrt(Var(X))


Covariance
----------

Covariance of discrete random variables X and Y:
  Cov(X, Y) = E[(X - E[X])(Y - E[Y])]
            = SUM SUM((xi - E[X])(yj - E[Y]) * p(xi, yj))

Correlation (normalized covariance):
  Cor(X, Y) = Cov(X, Y) / (SD(X) * SD(Y))
  
Ranges from -1 to 1.


Limit Theorems
--------------

Law of Large Numbers:
  Sample mean converges to population mean as sample size increases.
  
  LIM (n -> infinity) (X1 + X2 + ... + Xn) / n = mu

Central Limit Theorem:
  Standardized sum of independent random variables converges to normal distribution.


Entropy
-------

Entropy measures uncertainty in a random variable.

For discrete random variable X with probabilities p1, p2, ..., pn:
  H(X) = - SUM(pi * log_2(pi))

Measured in bits:
  1 bit = one binary choice
  H(X) = average bits needed to encode X


Mutual Information
------------------

Mutual information measures shared information between random variables.

For random variables X and Y:
  I(X; Y) = H(X) + H(Y) - H(X, Y)
  
where H(X, Y) is joint entropy.

Represents how much knowing Y reduces uncertainty about X.




